{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a312ecfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c81eeab",
   "metadata": {},
   "source": [
    "# The script is used to webscrape images from various sources for the Monster Idnetifier.\n",
    "\n",
    "This notebook supports the \"MonsterIdentifier\" notebook which outline the main summary of this project. This script simply contains the code I used to webscrape imagers from various wikipedia pages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36fee04",
   "metadata": {},
   "source": [
    "## Digimon Images\n",
    "\n",
    "The first set of images we scraped from are digimon monster images from the digimon wikipedia page: https://wikimon.net/Visual_List_of_Digimon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09436355",
   "metadata": {},
   "outputs": [],
   "source": [
    "digimon_DIR = \"digimon_images\" #Directory to store the images\n",
    "os.makedirs(digimon_DIR, exist_ok=True)\n",
    "\n",
    "Digi_URL = \"https://wikimon.net/Visual_List_of_Digimon\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72f51d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words to skip in image URLs\n",
    "SKIP_WORDS = [\"logo\", \"emblem\", \"Collectors\", \"Icon\", \"illustration\", \"Da-\", \"Bo-\"]\n",
    "\n",
    "downloaded_basenames = set()\n",
    "\n",
    "def get_all_images_from_main_page(page_url):\n",
    "    \"\"\"Get all valid images directly from the main Digimon list page.\"\"\"\n",
    "    try:\n",
    "        resp = requests.get(page_url)\n",
    "        resp.raise_for_status()\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "        img_tags = soup.select(\"img\")\n",
    "        img_urls = []\n",
    "\n",
    "        for img in img_tags:\n",
    "            src = img.get(\"src\")\n",
    "            if not src:\n",
    "                continue\n",
    "            url = urljoin(page_url, src)\n",
    "\n",
    "            # Skipwords\n",
    "            if any(word.lower() in url.lower() for word in SKIP_WORDS):\n",
    "                continue\n",
    "\n",
    "            #The main image/thumbnail shown on the page was a 120px version but we want the 240px\n",
    "            # Therefore we replace 120px (thumbnail) with 240px variant\n",
    "            url = re.sub(r'/\\d+px-', '/240px-', url)\n",
    "\n",
    "            img_urls.append(url)\n",
    "\n",
    "        return list(set(img_urls))  # remove duplicates\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching images from main page:\", e)\n",
    "        return []\n",
    "\n",
    "def download_image(url, folder):\n",
    "    \"\"\"Download image if it passes filters and hasn't been downloaded yet.\"\"\"\n",
    "    try:\n",
    "        path = urlparse(url).path\n",
    "        filename = os.path.basename(path)\n",
    "        filename = filename.split(\"?\")[0]\n",
    "        basename_no_ext = os.path.splitext(filename)[0]\n",
    "\n",
    "        if basename_no_ext in downloaded_basenames:\n",
    "            print(\"Already downloaded:\", basename_no_ext)\n",
    "            return\n",
    "\n",
    "        r = requests.get(url)\n",
    "        r.raise_for_status()\n",
    "        img = Image.open(BytesIO(r.content))\n",
    "\n",
    "        # Any image that's small we don;t want so just skip these\n",
    "        if img.width < 100 or img.height < 100:\n",
    "            print(\"Skipped small image:\", filename, f\"({img.width}x{img.height})\")\n",
    "            return\n",
    "\n",
    "        # Save safely\n",
    "        filename = re.sub(r'[^a-zA-Z0-9\\-_\\.]', '_', filename)\n",
    "        save_path = os.path.join(folder, filename)\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        img.save(save_path)\n",
    "        print(\"Downloaded:\", filename)\n",
    "        downloaded_basenames.add(basename_no_ext)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Failed to download\", url, e)\n",
    "\n",
    "def scrape_digimon_main_page():\n",
    "    img_urls = get_all_images_from_main_page(Digi_URL)\n",
    "    print(\"Found images on main page:\", len(img_urls))\n",
    "    for url in img_urls:\n",
    "        download_image(url, digimon_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f600cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment when you want to run the scraper\n",
    "\n",
    "#scrape_digimon_main_page()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d084ae15",
   "metadata": {},
   "source": [
    "## Monster Hunter scraper\n",
    "\n",
    "Next we scraped images of the various monsters in the monster hunter games from this wiki page: https://monsterhunter.fandom.com/wiki/Category:Monsters and this one: \"https://monsterhunter.fandom.com/wiki/Category:Photo_Galleries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738fe5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define URLS and directories for Monster Hunter\n",
    "#monsterHunter_URL = \"https://monsterhunter.fandom.com/wiki/Category:Monsters\"\n",
    "\n",
    "categories = [\n",
    "    \"https://monsterhunter.fandom.com/wiki/Category:Monsters\",\n",
    "    \"https://monsterhunter.fandom.com/wiki/Category:Photo_Galleries\"\n",
    "]\n",
    "\n",
    "monsterHunter_DIR = \"monsterHunter_images\"\n",
    "\n",
    "os.makedirs(monsterHunter_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e9d08e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_monster_links_from_category(category_url):\n",
    "    \"\"\"Get all monster page links from category page.\"\"\"\n",
    "    try:\n",
    "        resp = requests.get(category_url)\n",
    "        resp.raise_for_status()\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "        links = soup.select(\"div.category-page__members a[href^='/wiki/']\")\n",
    "        monster_links = []\n",
    "        for link in links:\n",
    "            href = link.get(\"href\")\n",
    "            if href and not href.startswith(\"/wiki/Category:\"):\n",
    "                full_url = urljoin(category_url, href)\n",
    "                monster_links.append(full_url)\n",
    "        print(f\"Found {len(monster_links)} monster links in category: {category_url}\")\n",
    "        return monster_links\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching monster links:\", e)\n",
    "        return []\n",
    "\n",
    "#I noticed that there were a lot of concept art or comic images. What I wanted were images with 'render' in the URL as they allhad the same style\n",
    "def get_all_images_from_monster_page(monster_url):\n",
    "    \"\"\"Get all image URLs from a monster page that contain 'render'.\"\"\"\n",
    "    try:\n",
    "        resp = requests.get(monster_url)\n",
    "        resp.raise_for_status()\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "        img_tags = []\n",
    "\n",
    "        # Try main profile image first, these are the 'nice' monster images we want\n",
    "        img_tags.extend(soup.select(\"aside.portable-infobox img\"))\n",
    "\n",
    "        # Try any image in content / gallery next\n",
    "        img_tags.extend(soup.select(\"div.mw-parser-output img\"))\n",
    "        img_tags.extend(soup.select(\"figure.pi-image-collection img\"))\n",
    "\n",
    "        img_urls = []\n",
    "        for img_tag in img_tags:\n",
    "            if img_tag and img_tag.get(\"src\"):\n",
    "                url = urljoin(monster_url, img_tag[\"src\"])\n",
    "                # Only include proper renders / large images (skip thumbnails)\n",
    "                if (\"scale-to-width-down\" in url or \"revision/latest\" in url) and \"render\" in url.lower():\n",
    "                    img_urls.append(url)\n",
    "\n",
    "        return list(set(img_urls))  # remove duplicates\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching images from monster page:\", monster_url, e)\n",
    "        return []\n",
    "\n",
    "def download_image(url, folder):\n",
    "    \"\"\"Download and save image from URL only if it doesn't already exist.\"\"\"\n",
    "    try:\n",
    "        path_parts = urlparse(url).path.split(\"/\")\n",
    "        filename = next((part for part in reversed(path_parts) if \".\" in part), \"image.png\")\n",
    "        filename = filename.split(\"?\")[0]\n",
    "        filename = re.sub(r'[^a-zA-Z0-9\\-_\\.]', '_', filename)\n",
    "\n",
    "        save_path = os.path.join(folder, filename)\n",
    "\n",
    "        if os.path.exists(save_path):\n",
    "            print(\"Already exists, skipping:\", filename)\n",
    "            return\n",
    "\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        img.save(save_path)\n",
    "        print(\"Downloaded:\", filename)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error downloading image:\", url, e)\n",
    "\n",
    "def webScrapeMH(categories, output_dir):\n",
    "    \"\"\"Scrape multiple categories for monster images.\"\"\"\n",
    "    for category_url in categories:\n",
    "        print(f\"Scraping category: {category_url}\")\n",
    "        monster_links = get_monster_links_from_category(category_url)\n",
    "\n",
    "        for monster_url in monster_links:\n",
    "            img_urls = get_all_images_from_monster_page(monster_url)\n",
    "            for img_url in img_urls:\n",
    "                download_image(img_url, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82d5057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# webScrapeMH(categories, monsterHunter_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8842167",
   "metadata": {},
   "source": [
    "## Palworld Scraper\n",
    "\n",
    "Next we webscrape palworld images from these wiki sites: https://palworld.fandom.com/wiki/Category:Images_-_Pals,\n",
    "\n",
    "    \"https://palworld.fandom.com/wiki/Alpha_Pals\",\n",
    "    \"https://palworld.fandom.com/wiki/Breeding\",\n",
    "    \"https://palworld.fandom.com/wiki/Lucky_Pals\",\n",
    "    \"https://palworld.fandom.com/wiki/Legendary_Pals\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bffcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Palword_DIR = \"palworld_images\"\n",
    "os.makedirs(Palword_DIR, exist_ok=True)\n",
    "\n",
    "palworld_URL = \"https://palworld.fandom.com/wiki/Category:Images_-_Pals\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e9e4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_category_pages(category_url):\n",
    "    \"\"\"Get all paginated category pages.\"\"\"\n",
    "    pages = [category_url]\n",
    "    while True:\n",
    "        resp = requests.get(pages[-1])\n",
    "        resp.raise_for_status()\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "        next_link = soup.select_one(\"a.category-page__pagination-next\")\n",
    "        if next_link and next_link.get(\"href\"):\n",
    "            next_page = urljoin(category_url, next_link[\"href\"])\n",
    "            if next_page not in pages:\n",
    "                pages.append(next_page)\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    return pages\n",
    "\n",
    "def get_file_page_urls_pal(category_url):\n",
    "    \"\"\"Get all file page URLs from a category (including all paginated pages).\"\"\"\n",
    "    file_urls = []\n",
    "    pages = get_all_category_pages(category_url)\n",
    "    print(f\"Found {len(pages)} pages in category\")\n",
    "\n",
    "    for page in pages:\n",
    "        resp = requests.get(page)\n",
    "        resp.raise_for_status()\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "        # more general: select all <a> inside the members section\n",
    "        for link in soup.select(\"div.category-page__members a\"):\n",
    "            href = link.get(\"href\")\n",
    "            if href and \"File:\" in href:\n",
    "                full_url = urljoin(category_url, href)\n",
    "                if full_url not in file_urls:\n",
    "                    file_urls.append(full_url)\n",
    "    return file_urls\n",
    "\n",
    "def get_high_res_image_url_pal(file_page_url):\n",
    "    \"\"\"Get the high-res image from a Fandom file page, handling redirects like ?file=.\"\"\"\n",
    "    resp = requests.get(file_page_url)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    \n",
    "    # Try <div class=\"fullMedia\"> first\n",
    "    img_link = soup.select_one(\"div.fullMedia a\")\n",
    "    if img_link and img_link.get(\"href\"):\n",
    "        return img_link[\"href\"]\n",
    "    \n",
    "    # fallback: <a class=\"image\">\n",
    "    img_link = soup.select_one(\"a.image\")\n",
    "    if img_link and img_link.get(\"href\"):\n",
    "        return urljoin(file_page_url, img_link[\"href\"])\n",
    "    \n",
    "    # fallback: first <img> tag\n",
    "    img_tag = soup.select_one(\"img\")\n",
    "    if img_tag and img_tag.get(\"src\"):\n",
    "        return urljoin(file_page_url, img_tag[\"src\"])\n",
    "    \n",
    "    print(\"No high-res image found for:\", file_page_url)\n",
    "    return None\n",
    "\n",
    "def download_image_pal(url, folder):\n",
    "    \"\"\"Download and save image from URL.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "\n",
    "        path_parts = urlparse(url).path.split(\"/\")\n",
    "        filename = next((part for part in reversed(path_parts) if \".\" in part), \"image.png\")\n",
    "        filename = filename.split(\"?\")[0]\n",
    "        filename = re.sub(r'[^a-zA-Z0-9\\-_\\.]', '_', filename)\n",
    "\n",
    "        save_path = os.path.join(folder, filename)\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        img.save(save_path)\n",
    "        print(\"Downloaded:\", filename)\n",
    "    except Exception as e:\n",
    "        print(\"Error downloading\", url, e)\n",
    "\n",
    "def webScrapePal(BASE_URL, OUTPUT_DIR):\n",
    "    file_pages = get_file_page_urls_pal(BASE_URL)\n",
    "    print(f\"Found {len(file_pages)} file pages total\")\n",
    "\n",
    "    for file_page in file_pages:\n",
    "        img_url = get_high_res_image_url_pal(file_page)\n",
    "        if img_url:\n",
    "            download_image_pal(img_url, OUTPUT_DIR)\n",
    "        else:\n",
    "            print(\"No high-res image found for:\", file_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0b0122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 pages in category\n",
      "Found 267 file pages total\n",
      "Downloaded: Anubis.png\n",
      "Downloaded: Anubis.png\n",
      "Downloaded: Nitewing.png\n",
      "Downloaded: Arsox.png\n",
      "Downloaded: Astegon.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Azurobe.png\n",
      "Downloaded: Azurobe.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Leather_icon.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Beegarde.png\n",
      "Downloaded: Leather_icon.png\n",
      "Downloaded: Leather_icon.png\n",
      "Downloaded: Leather_icon.png\n",
      "Downloaded: Boltmane.png\n",
      "Downloaded: Bristla.png\n",
      "Downloaded: Broncherry_Aqua.png\n",
      "Downloaded: Broncherry.png\n",
      "Downloaded: Bushi.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Caprity.png\n",
      "Downloaded: Cattiva.png\n",
      "Downloaded: Cawgnito.png\n",
      "Downloaded: Cawgnito.png\n",
      "Downloaded: Cawgnito.png\n",
      "Downloaded: Celaray.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Celaray.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Chikipi.png\n",
      "Downloaded: Chikipi.png\n",
      "Downloaded: Pals.png\n",
      "Downloaded: Chikipi.png\n",
      "Downloaded: Chillet.png\n",
      "Downloaded: Cinnamoth.png\n",
      "Downloaded: Colossal_Whale.png\n",
      "Downloaded: Colossal_Wyrm.png\n",
      "Downloaded: Colossal_Wyrm.png\n",
      "Downloaded: Cremis.png\n",
      "Downloaded: Cremis.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Cryolinx.png\n",
      "Downloaded: Daedream.png\n",
      "Downloaded: Dazzi.png\n",
      "Downloaded: Depresso.png\n",
      "Downloaded: Digtoise.png\n",
      "Downloaded: Digtoise.png\n",
      "Downloaded: Digtoise.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Dinossom.png\n",
      "Downloaded: Dinossom.png\n",
      "Downloaded: Direhowl.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Dragostrophe_27s_Cousin.png\n",
      "Downloaded: Dragostrophe.png\n",
      "Downloaded: Dumud.png\n",
      "Downloaded: Leather_icon.png\n",
      "Downloaded: Eikthyrdeer.png\n",
      "Downloaded: Eikthyrdeer.png\n",
      "Downloaded: Elizabee.png\n",
      "Downloaded: Elizabee.png\n",
      "Downloaded: Leather_icon.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Elphidran.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Faleris.png\n",
      "Downloaded: Faleris.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Felbat.png\n",
      "Downloaded: Fenglope.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Flambelle.png\n",
      "Downloaded: Flambelle.png\n",
      "Downloaded: Flambelle.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Flopie.png\n",
      "Downloaded: Frostallion.png\n",
      "Downloaded: Foxcicle.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Foxparks.png\n",
      "Downloaded: Foxparks.png\n",
      "Downloaded: Foxparks.png\n",
      "Downloaded: Leather_icon.png\n",
      "Downloaded: Frostallion.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Water.png\n",
      "Downloaded: Fuack.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Fuddler.png\n",
      "Downloaded: Galeclaw.png\n",
      "Downloaded: Galeclaw.png\n",
      "Downloaded: Galeclaw.png\n",
      "Downloaded: Gobfin.png\n",
      "Downloaded: Gorirat.png\n",
      "Downloaded: Ranch.png\n",
      "Downloaded: Leather_icon.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Grintale.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Grizzbolt.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Gumoss.png\n",
      "Downloaded: Gumoss.png\n",
      "Downloaded: Hangyu.png\n",
      "Downloaded: Leather_icon.png\n",
      "Downloaded: Hangyu.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Leather_icon.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Hoocrates.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Incineram_Noct.png\n",
      "Downloaded: Incineram.png\n",
      "Downloaded: Jetragon.png\n",
      "Downloaded: Jetragon.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Jolthog.png\n",
      "Downloaded: Leather_icon.png\n",
      "Downloaded: Jormuntide.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Katress.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Kelpsea_Ignis.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Kelpsea.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Killamari.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Leather_icon.png\n",
      "Downloaded: Kingpaca.png\n",
      "Downloaded: Kingpaca.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Kitsun.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Leezpunk_Ignis.png\n",
      "Downloaded: Leezpunk.png\n",
      "Downloaded: Lifmunk.png\n",
      "Downloaded: Loupmoon.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Lovander.png\n",
      "Downloaded: Bristla.png\n",
      "Downloaded: Help_Guide_40.png\n",
      "Downloaded: Help_Guide_40.png\n",
      "Downloaded: Lunaris.png\n",
      "Downloaded: Lunaris.png\n",
      "Downloaded: Lyleen_Noct.png\n",
      "Downloaded: Lyleen.png\n",
      "Downloaded: Lyleen.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Leather_icon.png\n",
      "Downloaded: Mammorest.png\n",
      "Downloaded: Maraith.png\n",
      "Downloaded: Mau_Cryst.png\n",
      "Downloaded: Mau.png\n",
      "Downloaded: Mau_Cryst.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Melpaca.png\n",
      "Downloaded: Melpaca.png\n",
      "Downloaded: Menasting.png\n",
      "Downloaded: Mossanda_Lux.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Mossanda.png\n",
      "Downloaded: Mossanda.png\n",
      "Downloaded: Mozzarina.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Mozzarina.png\n",
      "Downloaded: Mozzarina.png\n",
      "Downloaded: Leather_icon.png\n",
      "Downloaded: Nitewing.png\n",
      "Downloaded: Nitewing.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Nox.png\n",
      "Downloaded: Orserk.png\n",
      "Downloaded: Leather_icon.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Pengullet.png\n",
      "Downloaded: Pengullet.png\n",
      "Downloaded: Pengullet.png\n",
      "Downloaded: Penking.png\n",
      "Downloaded: Petallia.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Pyrin.png\n",
      "Downloaded: Leather_icon.png\n",
      "Downloaded: Pyrin.png\n",
      "Downloaded: Pyrin.png\n",
      "Downloaded: Leather_icon.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Ragnahawk.png\n",
      "Downloaded: Rayhound.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Leather_icon.png\n",
      "Downloaded: Stone_Axe.png\n",
      "Downloaded: Relaxaurus.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Leather_icon.png\n",
      "Downloaded: Reptyro.png\n",
      "Downloaded: Ribbuny.png\n",
      "Downloaded: Ribbuny.png\n",
      "Downloaded: Robinquill_Terra.png\n",
      "Downloaded: Robinquill.png\n",
      "Downloaded: Rooby.png\n",
      "Downloaded: Rooby.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Rushoar.png\n",
      "Downloaded: Robinquill.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Shadowbeak.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Sibelyx.png\n",
      "Downloaded: Sibelyx.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Sparkit.png\n",
      "Downloaded: Sparkit.png\n",
      "Downloaded: Leather_icon.png\n",
      "Downloaded: Leather_icon.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Surfent.png\n",
      "Downloaded: Surfent.png\n",
      "Downloaded: Leather_icon.png\n",
      "Downloaded: Suzaku.png\n",
      "Downloaded: Suzaku.png\n",
      "Downloaded: Suzaku.png\n",
      "Downloaded: Swee.png\n",
      "Downloaded: Leather_icon.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Tanzee.png\n",
      "Downloaded: Tanzee.png\n",
      "Downloaded: Teafant.png\n",
      "Downloaded: Tocotoco.png\n",
      "Downloaded: Tocotoco.png\n",
      "Downloaded: Tombat.png\n",
      "Downloaded: Univolt.png\n",
      "Downloaded: Vaelet.png\n",
      "Downloaded: Leather_icon.png\n",
      "Downloaded: Leather_icon.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Verdash.png\n",
      "Downloaded: Vixy.png\n",
      "Downloaded: Warsect.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Warsect.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Warsect.png\n",
      "Downloaded: Chillet.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Wixen.png\n",
      "Downloaded: Woolipop.png\n",
      "Downloaded: TerrariaPalworld.png\n",
      "Downloaded: Leather_icon.png\n",
      "Downloaded: Wumpo.png\n"
     ]
    }
   ],
   "source": [
    "webScrapePal(palworld_URL, Palword_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec64e9c",
   "metadata": {},
   "source": [
    "alternate palworld image locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c1e06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(url, folder):\n",
    "    \"\"\"Download and save image from URL only if it doesn't already exist.\"\"\"\n",
    "    try:\n",
    "        # Extract filename from URL\n",
    "        path_parts = urlparse(url).path.split(\"/\")\n",
    "        filename = None\n",
    "        for part in reversed(path_parts):\n",
    "            if \".\" in part:\n",
    "                filename = part\n",
    "                break\n",
    "        if not filename:\n",
    "            filename = \"image.png\"\n",
    "\n",
    "        filename = filename.split(\"?\")[0]\n",
    "        filename = re.sub(r'[^a-zA-Z0-9\\-_\\.]', '_', filename)\n",
    "\n",
    "        save_path = os.path.join(folder, filename)\n",
    "\n",
    "        # Skip download if already exists\n",
    "        if os.path.exists(save_path):\n",
    "            print(\"Already exists, skipping:\", filename)\n",
    "            return\n",
    "\n",
    "        # Download image\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        img.save(save_path)\n",
    "        print(\"Downloaded:\", filename)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error downloading\", url, e)\n",
    "\n",
    "def get_pal_links_from_table(page_url):\n",
    "    \"\"\"Get all Pal page links from the main table on a page like Alpha_Pals.\"\"\"\n",
    "    try:\n",
    "        resp = requests.get(page_url)\n",
    "        resp.raise_for_status()\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "        # The table has links to individual Pal pages\n",
    "        links = soup.select(\"table a[href^='/wiki/']\")\n",
    "        pal_links = []\n",
    "        for link in links:\n",
    "            href = link.get(\"href\")\n",
    "            if href and not href.startswith(\"/wiki/Category:\"):\n",
    "                full_url = urljoin(page_url, href)\n",
    "                pal_links.append(full_url)\n",
    "\n",
    "        print(f\"Found {len(pal_links)} Pal links on page: {page_url}\")\n",
    "        return pal_links\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error getting Pal links from table:\", e)\n",
    "        return []\n",
    "\n",
    "def get_main_image_from_pal_page(pal_page_url):\n",
    "    \"\"\"Get the main profile image URL from a Pal page.\"\"\"\n",
    "    try:\n",
    "        resp = requests.get(pal_page_url)\n",
    "        resp.raise_for_status()\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "        # Usually the main image is in <aside class=\"portable-infobox\"> or first <img> in content\n",
    "        img_tag = soup.select_one(\"aside.portable-infobox img\")\n",
    "        if not img_tag:\n",
    "            img_tag = soup.select_one(\"div.mw-parser-output img\")\n",
    "\n",
    "        if img_tag and img_tag.get(\"src\"):\n",
    "            return urljoin(pal_page_url, img_tag[\"src\"])\n",
    "        else:\n",
    "            print(\"No image found for:\", pal_page_url)\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error getting image from Pal page:\", pal_page_url, e)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a34e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start scraping\n",
    "pal_pages_urls = [\n",
    "    \"https://palworld.fandom.com/wiki/Alpha_Pals\",\n",
    "    \"https://palworld.fandom.com/wiki/Breeding\",\n",
    "    \"https://palworld.fandom.com/wiki/Lucky_Pals\",\n",
    "    \"https://palworld.fandom.com/wiki/Legendary_Pals\"\n",
    "]\n",
    "\n",
    "for page_url in pal_pages_urls:\n",
    "    print(f\"Scraping page: {page_url}\")\n",
    "    pal_links = get_pal_links_from_table(page_url)\n",
    "    \n",
    "    for pal_url in pal_links:\n",
    "        img_url = get_main_image_from_pal_page(pal_url)\n",
    "        if img_url:\n",
    "            #download_image(img_url, Palword_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0699d9",
   "metadata": {},
   "source": [
    "# yugipedia Images\n",
    "Next we get a bunch more yugio images. We get them from this wiki site: https://yugipedia.com/wiki/Category:Yu-Gi-Oh!_Duel_Links_monster_images using the selenium package. We only grab images from the first 3 pages of this site (more than enough images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43ed88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment the url you want to scrape from. The each represent the next page of the wiki site.\n",
    "urls = [\n",
    "    #\"https://yugipedia.com/wiki/Category:Yu-Gi-Oh!_Duel_Links_monster_images\",\n",
    "    #\"https://yugipedia.com/index.php?title=Category:Yu-Gi-Oh!_Duel_Links_monster_images&filefrom=ArcanaForceXXITheWorld-DULI-EN-VG-NC.png#mw-category-media\",\n",
    "    #\"https://yugipedia.com/index.php?title=Category:Yu-Gi-Oh!_Duel_Links_monster_images&filefrom=BerserkGorilla-DULI-EN-VG-NC.png#mw-category-media\"\n",
    "]\n",
    "\n",
    "yugioh_DIR = \"yugipedia_images\"\n",
    "\n",
    "os.makedirs(yugioh_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d2bc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# --- Selenium setup ---\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.set_page_load_timeout(3000) \n",
    "\n",
    "folder = yugioh_DIR\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "for url in urls:\n",
    "    print(\"Scraping page:\", url)\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        # --- Find all images ---\n",
    "        gallery_items = soup.find_all(\"li\", class_=\"gallerybox\")\n",
    "        for item in gallery_items:\n",
    "            img_tag = item.find(\"img\")\n",
    "            if img_tag:\n",
    "                img_url = img_tag.get(\"src\")\n",
    "                # Convert relative URL to absolute if needed\n",
    "                if img_url.startswith(\"//\"):\n",
    "                    img_url = \"https:\" + img_url\n",
    "                \n",
    "                # Get filename from URL\n",
    "                filename = os.path.join(folder, img_url.split(\"/\")[-1].split(\"?\")[0])\n",
    "                \n",
    "                # Skip if already downloaded\n",
    "                if os.path.exists(filename):\n",
    "                    print(\"Already exists:\", filename)\n",
    "                    continue\n",
    "\n",
    "                # Download image\n",
    "                try:\n",
    "                    response = requests.get(img_url, stream=True)\n",
    "                    if response.status_code == 200:\n",
    "                        with open(filename, 'wb') as f:\n",
    "                            for chunk in response.iter_content(1024):\n",
    "                                f.write(chunk)\n",
    "                        print(f\"Downloaded: {filename}\")\n",
    "                    else:\n",
    "                        print(f\"Failed to download {img_url}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error downloading {img_url}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching page {url}: {e}\")\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bf84e6",
   "metadata": {},
   "source": [
    "With that, we now have a bunch of images from various sources and monster genres. Ideally, we want images with a creature/mosnter at the center with a white or transparent background. However these webscrapers definetly picked up some additional iamges, which is why I then manually went through each folder and just deleted images that looked wrong (concept art, comic images, other stuff)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a443c3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
